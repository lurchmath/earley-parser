{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Earley Parser Documentation Get started now To import this code via a CDN or npm , check out the API Reference Page . Background The Earley Parsing algorithm can parse strings according to any context-free grammar. This module lets the user define tokens and grammar rules, then pass strings in to be tokenized and parsed, or just pass in pre-tokenized arrays to be parsed. Parse trees are returned as nested JavaScript arrays, but the user can also provide callbacks that construct other kinds of hierarchical structures (or do computation) instead of building parse trees. More Information The following additional information is available in this documentation. Source Code - see the source and/or import it into your own project API Reference - how to use the various functions and objects provided More documentation may be added here in the future. For now you can view this repository .","title":"Home"},{"location":"#earley-parser-documentation","text":"","title":"Earley Parser Documentation"},{"location":"#get-started-now","text":"To import this code via a CDN or npm , check out the API Reference Page .","title":"Get started now"},{"location":"#background","text":"The Earley Parsing algorithm can parse strings according to any context-free grammar. This module lets the user define tokens and grammar rules, then pass strings in to be tokenized and parsed, or just pass in pre-tokenized arrays to be parsed. Parse trees are returned as nested JavaScript arrays, but the user can also provide callbacks that construct other kinds of hierarchical structures (or do computation) instead of building parse trees.","title":"Background"},{"location":"#more-information","text":"The following additional information is available in this documentation. Source Code - see the source and/or import it into your own project API Reference - how to use the various functions and objects provided More documentation may be added here in the future. For now you can view this repository .","title":"More Information"},{"location":"api-reference/","text":"API Reference Getting started In the browser Import the module, which you can download from our repository directly or import from a CDN with the following one-liner. <script src='https://cdn.jsdelivr.net/npm/earley-parser@1/earley-parser.js'></script> From the command line Or install this package into your project the usual way: npm install earley-parser Then within any of your modules, import it as follows. import { Tokenizer, Grammar } from 'earley-parser' After that, any of the example code snippets in this documentation should function as-is. In a WebWorker To place this script in a WebWorker, you will need to download its source file and place it in your project's web space. Your script can then create the worker as follows. W = new Worker( 'path/to/earley-parser.js', { type : 'module' } ); This exposes an asynchronous API documented below . Tokenizing Traditionally, parsing text is split first into a \"tokenization\" phase, in which chunks of text are recognized as atomic units, and thus the string becomes an array of its substrings, each of size 1 or greater, followed by the parsing phase, which operates on that flat array, arranging it into a tree structure. For example, in standard arithmetic, the text \"1+23*5\" might first be tokenized into the array [ \"1\", \"+\", \"23\", \"*\", \"5\" ] , and then parsed into the (prefix-notation) tree structure [ \"+\", \"1\", [ \"*\", \"23\", \"5\" ] ] . A tokenizer is therefore an ordered list of regular expressions for detecting tokens, popping them off of an input string, and then possibly manipulating them before adding them to a growing array of tokens found in the string. This module provides a class for creating tokenizers. Constructor The constructor takes no parameters, so it is very simple to use. T = new Tokenizer(); Adding types to a tokenizer There is precisely one function for adding types to the tokenizer. Note, however, that types are checked in the order in which they were added to the tokenizer, so when you call this function repeatedly to add various types of tokens, you should take care to order the calls correctly. For instance, perhaps you are writing a tokenizer for simple algebraic expressions, in which a sequence of letters will be seen as the multiplication of variables (i.e., \"abc\" means \"a times b times c\") except for a few special sequences of letters such as \"sin\" and \"cos\" and \"pi\" and perhaps a few others. It is important to add the tokens for \"sin\" and \"cos\" and so on first, and then the generic single-letter token thereafter, so that the special cases have a chance to be applied before the general case. Otherwise the general case would catch all sequences of letters, and the special cases would never have a chance to be applied. The function signature looks like so: T.addType( regexp, formatter ); The two parameters are documented thoroughly in the source code documentation , so I do not repeat the information here. Tokenizing To tokenize text, simply call T.tokenize( input ) on the text. Example: T = new Tokenizer(); T.addType( /sin/ ); T.addType( /cos/ ); T.addType( /pi/ ); T.addType( /[a-z]/, function ( name ) { return \"Variable:\" + name; } ); T.addType( /\\s/, function () { return null; } ); console.log( T.tokenize( 'sin x' ) ); console.log( T.tokenize( 'cospiy' ) ); Parsing A grammar is a set of rules defining the language to parse. For more information on context-free grammars, see the Wikipedia article on the Earley parser . For the sake of having a concrete running example in this section, let's assume we want to want to create the extremely simple grammar used as an example in that same article . We will make one modification: we will accept any integer, rather than just the four digits in that example. Our grammar can thus be summarized as the following rules. P ::= S S ::= S+M | M M ::= M*T | T T ::= any integer We can represent the same grammar without the | symbol by separating single lines into two separate lines. P ::= S S ::= S+M S ::= M M ::= M*T M ::= T T ::= any integer Either way of representing a grammar is acceptable, and supported by this module. See the section on specifying grammar rules , below. Constructor There is just one constructor for grammars, and it takes as its sole argument the name of the start nonterminal. This need not be a single capital letter, as it is in the example above; nonterminals can have any word as their name. G = new Grammar( 'P' ); Setting default options After constructing a new grammar, you can choose to set some default options that will govern its behavior. Any of these can be overridden in any call to the parse function, later, but you can set defaults here if you plan to need them often. You call G.setOption( name, value ) to set the default value for any option. The options are documented thoroughly in the source code documentation , so I do not repeat that information here. Examples of the output produced by the various options appears in the parsing section , below. Adding grammar rules A grammar rule requires a left-hand side, which must be a single nonterminal, represented by a string. Its right-hand side is typically an array. For instance, the grammar rule M ::= M*T has M as its left-hand side and the three-element array M , * , and T as its right hand side. The elements on the right hand side come in two types. There are other nonterminals (such as M and T) and there are terminals (the symbol * , in this example). Nonterminals are represented as strings, and terminals as regular expressions. Thus to create the grammar rule M ::= M*T , we would use 'M' as the left-hand side and [ 'M', /\\*/, 'T' ] as the right-hand side. Our complete example grammar can then be created as follows. G = new Grammar( 'P' ); G.addRule( 'P', [ 'S' ] ); G.addRule( 'S', [ 'S', /\\+/, 'M' ] ); G.addRule( 'S', [ 'M' ] ); G.addRule( 'M', [ 'M', /\\*/, 'T' ] ); G.addRule( 'M', [ 'T' ] ); G.addRule( 'T', [ /-?[0-9]+/ ] ); There are a few things to improve upon here. We may combine rules that have the same left-hand side, just by listing the right-hand sides one after the other. We may express a one-element array containing a terminal by the regular expression alone, omitting the enclosing array. We may express a right-hand side consisting exclusively of one or more nonterminals by writing it as a single string, with the names of the nonterminals separated by spaces. This reduces our grammar somewhat. G = new Grammar( 'P' ); G.addRule( 'P', 'S' ); G.addRule( 'S', [ 'S', /\\+/, 'M' ], 'M' ); G.addRule( 'M', [ 'M', /\\*/, 'T' ], 'T' ); G.addRule( 'T', /-?[0-9]+/ ); The P in this grammar is redundant; we could have left it out and declared our grammar to start with S, but I leave it in to be consistent with the Wikipedia article from which it was taken. Running the parser We can then parse text by passing it to the grammar's parse member function. T = new Tokenizer(); T.addType( /-?[0-9]+/ ); T.addType( /[+*]/ ); T.addType( /\\s/, function () { return null; } ); G = new Grammar( 'P' ); G.setOption( 'tokenizer', T ); // Comment out either of these lines and re-run to see their effects: G.setOption( 'addCategories', false ); G.setOption( 'collapseBranches', true ); G.addRule( 'P', 'S' ); G.addRule( 'S', [ 'S', /\\+/, 'M' ], 'M' ); G.addRule( 'M', [ 'M', /\\*/, 'T' ], 'T' ); G.addRule( 'T', /-?[0-9]+/ ); JSON.stringify( G.parse( '15 + -2 * 9' ), null, 2 ); The result of the parse function is an array of valid parsings. In this case, it has only one element, because there is only one way the expression could be parsed. For more complex grammars with ambiguities, more than one result may be returned. To override options chosen with setOption() , call the parse function with an optional second argument, an object whose keys and values override the existing options. Example: G.parse( 'text', { collapseBranches : true } ); More Examples In addition to the brief examples shown in this file, the test suite in the source code repository is (naturally) a large set of examples of how the module works. WebWorker API This section assumes that you have read and understood the API for the non-WebWorker use of the module, as given in the previous sections. It also assumes that you've read the getting started section so that you know how to import this module into a WebWorker. Assuming you've created a worker W as in that section, you can then interact with it through five types of messages. Create a new parser W.postMessage( [ \"newParser\", \"name here\", \"start token here\" ] ); This creates a new parser keyed by the given name, with no tokenizer, overwriting any old one with the same name. Example: W.postMessage( [ \"newParser\", \"math expressions\", \"term\" ] ); // No rules in this parser yet, just the parser itself. Add a token type W.postMessage( [ \"addType\", \"parser name\", \"string of token regexp\", \"optional string of transform function code\" ] ); This adds a token type to the parser's tokenizer. If the parser does not yet have a tokenizer, this first creates a new one. When adding a type to a tokenizer (as documented above) we normally provide the regular expression for recognizing that token. But regular expressions cannot be passed to WebWorkers, and so just their source as a string should be passed instead. See the example below. Token types can come with an optional transform function to be applied to any token recognized of that type. That fourth parameter is optional and may be omitted from the form shown above. If present, it should be the string representation of the function, so it will survive passage to the WebWorker. See the example below. Example: whiteSpaceRegExp = /\\s+/; deleteWhiteSpace = function () { return null; } W.postMessage( [ \"addType\", \"math expressions\", whiteSpaceRegExp.source, String( deleteWhiteSpace ) ] ); Add a grammar rule W.postMessage( [ \"addRule\", \"parser name\", \"category\", sequences... ] ); This adds one or more rules to the parser. Sending a message of this type leads directly to function calls of addRule() as documented above. Thus you can structure your sequences the same as in calls to addRule() , except for the following change. Because regular expressions cannot be passed to WebWorkers, we modify the convention for the sequences. Where you would have represented a category by a string containing its name \"cat\" you now represent it by a string containing its name, plus the prefix to show that it is a category, \"c:cat\" . Where you would have represented a terminal by a regular expression that matches the terminal /fo+/ , you now represent it by a string containing the regular expression's source, plus the prefix to show that it is a terminal, \"t:fo+\" . Example: W.postMessage( [ \"addRule\", \"math expressions\", \"integer\", \"t:-?[0-9]+\" ] ); Parse text using a parser defined earlier W.onmessage = function ( event ) { console.log( \"Heard back from the worker with this:\", event.data ); } W.sendMessage( [ \"parser name\", \"text to parse\" ] ); This instructs the worker to parse the given text with the named parser, which must have been defined earlier by messages of the \"newParser\", \"addType\", and \"addRule\" types. The results are posted back to the main thread using postMessage() from within the worker thread, and thus the onmessage handler must be implemented, as shown in the example code above. The data in the event will be an array containing all valid parse trees for the given text in the grammar of the named parser. The trees are represented as nested JavaScript arrays whose first (\"head\") elements state the grammar categories contained in the grammar. For example, one parse tree might look like the following. (Keep in mind that even if this were the only valid parse tree, it would still be wrapped in another array to show that it was the one valid parsing of the given text.) [ \"sum\", [ \"integer\", \"5\" ], [ \"product\", [ \"integer\", \"6\" ], [ \"variable\", \"x\" ] ] ] If you choose not to have the module do tokenization for you, you can just not send any messages of the \"addToken\" type, and then pass an array of tokens in place of the string of text to parse. Example: var textToParse = \"12-6/x\"; W.onmessage = function ( event ) { console.log( \"The valid parse trees for\", textToParse, \"are:\" ); if ( event.data.length == 0 ) console.log( \"(none)\" ); else for ( var i = 0 ; i < event.data.length ; i++ ) console.log( (i+1) + \":\", event.data[i] ); } W.postMessage( [ \"parse\", \"math expression\", textToParse ] ); Delete an old parser W.postMessage( [ \"deleteParser\", \"name here\" ] ); Lets the worker reclaim memory by discarding parsers about which no further messages will be passed. Example: W.postMessage( [ \"deleteParser\", \"math expression\" ] ); var elements = document.getElementsByClassName( 'runnable-example' ); for ( var i = 0 ; i < elements.length ; i++ ) { var source = elements[i].textContent; elements[i].textContent = ''; var notebook = RunKit.createNotebook( { element: elements[i], source: source, preamble: 'Tokenizer = require( \"earley-parser\" ).Tokenizer;\\nGrammar = require( \"earley-parser\" ).Grammar;' } ); }","title":"Reference"},{"location":"api-reference/#api-reference","text":"","title":"API Reference"},{"location":"api-reference/#getting-started","text":"","title":"Getting started"},{"location":"api-reference/#in-the-browser","text":"Import the module, which you can download from our repository directly or import from a CDN with the following one-liner. <script src='https://cdn.jsdelivr.net/npm/earley-parser@1/earley-parser.js'></script>","title":"In the browser"},{"location":"api-reference/#from-the-command-line","text":"Or install this package into your project the usual way: npm install earley-parser Then within any of your modules, import it as follows. import { Tokenizer, Grammar } from 'earley-parser' After that, any of the example code snippets in this documentation should function as-is.","title":"From the command line"},{"location":"api-reference/#in-a-webworker","text":"To place this script in a WebWorker, you will need to download its source file and place it in your project's web space. Your script can then create the worker as follows. W = new Worker( 'path/to/earley-parser.js', { type : 'module' } ); This exposes an asynchronous API documented below .","title":"In a WebWorker"},{"location":"api-reference/#tokenizing","text":"Traditionally, parsing text is split first into a \"tokenization\" phase, in which chunks of text are recognized as atomic units, and thus the string becomes an array of its substrings, each of size 1 or greater, followed by the parsing phase, which operates on that flat array, arranging it into a tree structure. For example, in standard arithmetic, the text \"1+23*5\" might first be tokenized into the array [ \"1\", \"+\", \"23\", \"*\", \"5\" ] , and then parsed into the (prefix-notation) tree structure [ \"+\", \"1\", [ \"*\", \"23\", \"5\" ] ] . A tokenizer is therefore an ordered list of regular expressions for detecting tokens, popping them off of an input string, and then possibly manipulating them before adding them to a growing array of tokens found in the string. This module provides a class for creating tokenizers.","title":"Tokenizing"},{"location":"api-reference/#constructor","text":"The constructor takes no parameters, so it is very simple to use. T = new Tokenizer();","title":"Constructor"},{"location":"api-reference/#adding-types-to-a-tokenizer","text":"There is precisely one function for adding types to the tokenizer. Note, however, that types are checked in the order in which they were added to the tokenizer, so when you call this function repeatedly to add various types of tokens, you should take care to order the calls correctly. For instance, perhaps you are writing a tokenizer for simple algebraic expressions, in which a sequence of letters will be seen as the multiplication of variables (i.e., \"abc\" means \"a times b times c\") except for a few special sequences of letters such as \"sin\" and \"cos\" and \"pi\" and perhaps a few others. It is important to add the tokens for \"sin\" and \"cos\" and so on first, and then the generic single-letter token thereafter, so that the special cases have a chance to be applied before the general case. Otherwise the general case would catch all sequences of letters, and the special cases would never have a chance to be applied. The function signature looks like so: T.addType( regexp, formatter ); The two parameters are documented thoroughly in the source code documentation , so I do not repeat the information here.","title":"Adding types to a tokenizer"},{"location":"api-reference/#tokenizing_1","text":"To tokenize text, simply call T.tokenize( input ) on the text. Example: T = new Tokenizer(); T.addType( /sin/ ); T.addType( /cos/ ); T.addType( /pi/ ); T.addType( /[a-z]/, function ( name ) { return \"Variable:\" + name; } ); T.addType( /\\s/, function () { return null; } ); console.log( T.tokenize( 'sin x' ) ); console.log( T.tokenize( 'cospiy' ) );","title":"Tokenizing"},{"location":"api-reference/#parsing","text":"A grammar is a set of rules defining the language to parse. For more information on context-free grammars, see the Wikipedia article on the Earley parser . For the sake of having a concrete running example in this section, let's assume we want to want to create the extremely simple grammar used as an example in that same article . We will make one modification: we will accept any integer, rather than just the four digits in that example. Our grammar can thus be summarized as the following rules. P ::= S S ::= S+M | M M ::= M*T | T T ::= any integer We can represent the same grammar without the | symbol by separating single lines into two separate lines. P ::= S S ::= S+M S ::= M M ::= M*T M ::= T T ::= any integer Either way of representing a grammar is acceptable, and supported by this module. See the section on specifying grammar rules , below.","title":"Parsing"},{"location":"api-reference/#constructor_1","text":"There is just one constructor for grammars, and it takes as its sole argument the name of the start nonterminal. This need not be a single capital letter, as it is in the example above; nonterminals can have any word as their name. G = new Grammar( 'P' );","title":"Constructor"},{"location":"api-reference/#setting-default-options","text":"After constructing a new grammar, you can choose to set some default options that will govern its behavior. Any of these can be overridden in any call to the parse function, later, but you can set defaults here if you plan to need them often. You call G.setOption( name, value ) to set the default value for any option. The options are documented thoroughly in the source code documentation , so I do not repeat that information here. Examples of the output produced by the various options appears in the parsing section , below.","title":"Setting default options"},{"location":"api-reference/#adding-grammar-rules","text":"A grammar rule requires a left-hand side, which must be a single nonterminal, represented by a string. Its right-hand side is typically an array. For instance, the grammar rule M ::= M*T has M as its left-hand side and the three-element array M , * , and T as its right hand side. The elements on the right hand side come in two types. There are other nonterminals (such as M and T) and there are terminals (the symbol * , in this example). Nonterminals are represented as strings, and terminals as regular expressions. Thus to create the grammar rule M ::= M*T , we would use 'M' as the left-hand side and [ 'M', /\\*/, 'T' ] as the right-hand side. Our complete example grammar can then be created as follows. G = new Grammar( 'P' ); G.addRule( 'P', [ 'S' ] ); G.addRule( 'S', [ 'S', /\\+/, 'M' ] ); G.addRule( 'S', [ 'M' ] ); G.addRule( 'M', [ 'M', /\\*/, 'T' ] ); G.addRule( 'M', [ 'T' ] ); G.addRule( 'T', [ /-?[0-9]+/ ] ); There are a few things to improve upon here. We may combine rules that have the same left-hand side, just by listing the right-hand sides one after the other. We may express a one-element array containing a terminal by the regular expression alone, omitting the enclosing array. We may express a right-hand side consisting exclusively of one or more nonterminals by writing it as a single string, with the names of the nonterminals separated by spaces. This reduces our grammar somewhat. G = new Grammar( 'P' ); G.addRule( 'P', 'S' ); G.addRule( 'S', [ 'S', /\\+/, 'M' ], 'M' ); G.addRule( 'M', [ 'M', /\\*/, 'T' ], 'T' ); G.addRule( 'T', /-?[0-9]+/ ); The P in this grammar is redundant; we could have left it out and declared our grammar to start with S, but I leave it in to be consistent with the Wikipedia article from which it was taken.","title":"Adding grammar rules"},{"location":"api-reference/#running-the-parser","text":"We can then parse text by passing it to the grammar's parse member function. T = new Tokenizer(); T.addType( /-?[0-9]+/ ); T.addType( /[+*]/ ); T.addType( /\\s/, function () { return null; } ); G = new Grammar( 'P' ); G.setOption( 'tokenizer', T ); // Comment out either of these lines and re-run to see their effects: G.setOption( 'addCategories', false ); G.setOption( 'collapseBranches', true ); G.addRule( 'P', 'S' ); G.addRule( 'S', [ 'S', /\\+/, 'M' ], 'M' ); G.addRule( 'M', [ 'M', /\\*/, 'T' ], 'T' ); G.addRule( 'T', /-?[0-9]+/ ); JSON.stringify( G.parse( '15 + -2 * 9' ), null, 2 ); The result of the parse function is an array of valid parsings. In this case, it has only one element, because there is only one way the expression could be parsed. For more complex grammars with ambiguities, more than one result may be returned. To override options chosen with setOption() , call the parse function with an optional second argument, an object whose keys and values override the existing options. Example: G.parse( 'text', { collapseBranches : true } );","title":"Running the parser"},{"location":"api-reference/#more-examples","text":"In addition to the brief examples shown in this file, the test suite in the source code repository is (naturally) a large set of examples of how the module works.","title":"More Examples"},{"location":"api-reference/#webworker-api","text":"This section assumes that you have read and understood the API for the non-WebWorker use of the module, as given in the previous sections. It also assumes that you've read the getting started section so that you know how to import this module into a WebWorker. Assuming you've created a worker W as in that section, you can then interact with it through five types of messages.","title":"WebWorker API"},{"location":"api-reference/#create-a-new-parser","text":"W.postMessage( [ \"newParser\", \"name here\", \"start token here\" ] ); This creates a new parser keyed by the given name, with no tokenizer, overwriting any old one with the same name. Example: W.postMessage( [ \"newParser\", \"math expressions\", \"term\" ] ); // No rules in this parser yet, just the parser itself.","title":"Create a new parser"},{"location":"api-reference/#add-a-token-type","text":"W.postMessage( [ \"addType\", \"parser name\", \"string of token regexp\", \"optional string of transform function code\" ] ); This adds a token type to the parser's tokenizer. If the parser does not yet have a tokenizer, this first creates a new one. When adding a type to a tokenizer (as documented above) we normally provide the regular expression for recognizing that token. But regular expressions cannot be passed to WebWorkers, and so just their source as a string should be passed instead. See the example below. Token types can come with an optional transform function to be applied to any token recognized of that type. That fourth parameter is optional and may be omitted from the form shown above. If present, it should be the string representation of the function, so it will survive passage to the WebWorker. See the example below. Example: whiteSpaceRegExp = /\\s+/; deleteWhiteSpace = function () { return null; } W.postMessage( [ \"addType\", \"math expressions\", whiteSpaceRegExp.source, String( deleteWhiteSpace ) ] );","title":"Add a token type"},{"location":"api-reference/#add-a-grammar-rule","text":"W.postMessage( [ \"addRule\", \"parser name\", \"category\", sequences... ] ); This adds one or more rules to the parser. Sending a message of this type leads directly to function calls of addRule() as documented above. Thus you can structure your sequences the same as in calls to addRule() , except for the following change. Because regular expressions cannot be passed to WebWorkers, we modify the convention for the sequences. Where you would have represented a category by a string containing its name \"cat\" you now represent it by a string containing its name, plus the prefix to show that it is a category, \"c:cat\" . Where you would have represented a terminal by a regular expression that matches the terminal /fo+/ , you now represent it by a string containing the regular expression's source, plus the prefix to show that it is a terminal, \"t:fo+\" . Example: W.postMessage( [ \"addRule\", \"math expressions\", \"integer\", \"t:-?[0-9]+\" ] );","title":"Add a grammar rule"},{"location":"api-reference/#parse-text-using-a-parser-defined-earlier","text":"W.onmessage = function ( event ) { console.log( \"Heard back from the worker with this:\", event.data ); } W.sendMessage( [ \"parser name\", \"text to parse\" ] ); This instructs the worker to parse the given text with the named parser, which must have been defined earlier by messages of the \"newParser\", \"addType\", and \"addRule\" types. The results are posted back to the main thread using postMessage() from within the worker thread, and thus the onmessage handler must be implemented, as shown in the example code above. The data in the event will be an array containing all valid parse trees for the given text in the grammar of the named parser. The trees are represented as nested JavaScript arrays whose first (\"head\") elements state the grammar categories contained in the grammar. For example, one parse tree might look like the following. (Keep in mind that even if this were the only valid parse tree, it would still be wrapped in another array to show that it was the one valid parsing of the given text.) [ \"sum\", [ \"integer\", \"5\" ], [ \"product\", [ \"integer\", \"6\" ], [ \"variable\", \"x\" ] ] ] If you choose not to have the module do tokenization for you, you can just not send any messages of the \"addToken\" type, and then pass an array of tokens in place of the string of text to parse. Example: var textToParse = \"12-6/x\"; W.onmessage = function ( event ) { console.log( \"The valid parse trees for\", textToParse, \"are:\" ); if ( event.data.length == 0 ) console.log( \"(none)\" ); else for ( var i = 0 ; i < event.data.length ; i++ ) console.log( (i+1) + \":\", event.data[i] ); } W.postMessage( [ \"parse\", \"math expression\", textToParse ] );","title":"Parse text using a parser defined earlier"},{"location":"api-reference/#delete-an-old-parser","text":"W.postMessage( [ \"deleteParser\", \"name here\" ] ); Lets the worker reclaim memory by discarding parsers about which no further messages will be passed. Example: W.postMessage( [ \"deleteParser\", \"math expression\" ] ); var elements = document.getElementsByClassName( 'runnable-example' ); for ( var i = 0 ; i < elements.length ; i++ ) { var source = elements[i].textContent; elements[i].textContent = ''; var notebook = RunKit.createNotebook( { element: elements[i], source: source, preamble: 'Tokenizer = require( \"earley-parser\" ).Tokenizer;\\nGrammar = require( \"earley-parser\" ).Grammar;' } ); }","title":"Delete an old parser"},{"location":"source-code/","text":"Source Code Reading the source The code in the repository resides in one file , written in JavaScript. Importing the source To import the source into your project, you can include it directly from a CDN at this URL .","title":"Source"},{"location":"source-code/#source-code","text":"","title":"Source Code"},{"location":"source-code/#reading-the-source","text":"The code in the repository resides in one file , written in JavaScript.","title":"Reading the source"},{"location":"source-code/#importing-the-source","text":"To import the source into your project, you can include it directly from a CDN at this URL .","title":"Importing the source"}]}
{
    "docs": [
        {
            "location": "/", 
            "text": "Earley Parser Documentation\n\n\nGet started now\n\n\nTo import this code via a CDN or \nnpm\n, check out the \nAPI Reference\nPage\n.\n\n\nBackground\n\n\nThe \nEarley Parsing algorithm\n\ncan parse strings according to any context-free grammar.  This module lets\nthe user define tokens and grammar rules, then pass strings in to be\ntokenized and parsed, or just pass in pre-tokenized arrays to be parsed.\n\n\nParse trees are returned as nested JavaScript arrays, but the user can also\nprovide callbacks that construct other kinds of hierarchical structures (or\ndo computation) instead of building parse trees.\n\n\nMore Information\n\n\nThe following additional information is available in this documentation.\n\n\n\n\nSource Code\n - see the source and/or import it into your\n   own project\n\n\nAPI Reference\n - how to use the various functions and\n   objects provided\n\n\n\n\nMore documentation may be added here in the future. For now you can \nview this repository \n.", 
            "title": "Home"
        }, 
        {
            "location": "/#earley-parser-documentation", 
            "text": "", 
            "title": "Earley Parser Documentation"
        }, 
        {
            "location": "/#get-started-now", 
            "text": "To import this code via a CDN or  npm , check out the  API Reference\nPage .", 
            "title": "Get started now"
        }, 
        {
            "location": "/#background", 
            "text": "The  Earley Parsing algorithm \ncan parse strings according to any context-free grammar.  This module lets\nthe user define tokens and grammar rules, then pass strings in to be\ntokenized and parsed, or just pass in pre-tokenized arrays to be parsed.  Parse trees are returned as nested JavaScript arrays, but the user can also\nprovide callbacks that construct other kinds of hierarchical structures (or\ndo computation) instead of building parse trees.", 
            "title": "Background"
        }, 
        {
            "location": "/#more-information", 
            "text": "The following additional information is available in this documentation.   Source Code  - see the source and/or import it into your\n   own project  API Reference  - how to use the various functions and\n   objects provided   More documentation may be added here in the future. For now you can  view this repository  .", 
            "title": "More Information"
        }, 
        {
            "location": "/source-code/", 
            "text": "Source Code\n\n\nReading the source\n\n\nThe code in \nthe repository\n\nresides in \none\nfile\n,\nwritten in \nLiterate CoffeeScript\n.\n\n\nChanging the source\n\n\nIf you don't like that language, you can always compile it directly to\nJavaScript with the following command.\n\n\ncoffee --compile earley-parser.litcoffee\n\n\n\n\nThis assumes that you've \ninstalled\nCoffeeScript\n and have the \nsource\nfile\n\naccessible.\n\n\nImporting the source\n\n\nTo import the source into your project, you can include it directly from a\nCDN at \nthis\nURL\n. There is a\nsource map file in the same folder that your browser should detect.", 
            "title": "Source"
        }, 
        {
            "location": "/source-code/#source-code", 
            "text": "", 
            "title": "Source Code"
        }, 
        {
            "location": "/source-code/#reading-the-source", 
            "text": "The code in  the repository \nresides in  one\nfile ,\nwritten in  Literate CoffeeScript .", 
            "title": "Reading the source"
        }, 
        {
            "location": "/source-code/#changing-the-source", 
            "text": "If you don't like that language, you can always compile it directly to\nJavaScript with the following command.  coffee --compile earley-parser.litcoffee  This assumes that you've  installed\nCoffeeScript  and have the  source\nfile \naccessible.", 
            "title": "Changing the source"
        }, 
        {
            "location": "/source-code/#importing-the-source", 
            "text": "To import the source into your project, you can include it directly from a\nCDN at  this\nURL . There is a\nsource map file in the same folder that your browser should detect.", 
            "title": "Importing the source"
        }, 
        {
            "location": "/api-reference/", 
            "text": "API Reference\n\n\nGetting started\n\n\nIn the browser\n\n\nImport the minified JavaScript, which you can \ndownload from our repository\ndirectly\n\nor import from a CDN with the following one-liner.\n\n\nscript src='https://cdn.jsdelivr.net/npm/earley-parser@1/earley-parser.js'\n/script\n\n\n\n\n\nFrom the command line\n\n\nOr install this package into your project the usual way:\n\n\nnpm install earley-parser\n\n\n\n\nThen within any of your modules, import it as follows.\n\n\nTokenizer = require( \nearley-parser\n ).Tokenizer;\nGrammar = require( \nearley-parser\n ).Grammar;\n\n\n\n\nAfter that, any of the example code snippets in this documentation should\nfunction as-is.\n\n\nTokenizing\n\n\nTraditionally, parsing text is split first into a \"tokenization\" phase, in\nwhich chunks of text are recognized as atomic units, and thus the string\nbecomes an array of its substrings, each of size 1 or greater, followed by\nthe parsing phase, which operates on that flat array, arranging it into a\ntree structure.\n\n\nFor example, in standard arithmetic, the text \"1+23*5\" might first be\ntokenized into the array \n[ \"1\", \"+\", \"23\", \"*\", \"5\" ]\n, and then parsed\ninto the (prefix-notation) tree structure\n\n[ \"+\", \"1\", [ \"*\", \"23\", \"5\" ] ]\n.\n\n\nA tokenizer is therefore an ordered list of regular expressions for\ndetecting tokens, popping them off of an input string, and then possibly\nmanipulating them before adding them to a growing array of tokens found in\nthe string.  This module provides a class for creating tokenizers.\n\n\nConstructor\n\n\nThe constructor takes no parameters, so it is very simple to use.\n\n\nT = new Tokenizer();\n\n\n\n\nAdding types to a tokenizer\n\n\nThere is precisely one function for adding types to the tokenizer.  Note,\nhowever, that types are checked in the order in which they were added to the\ntokenizer, so when you call this function repeatedly to add various types of\ntokens, you should take care to order the calls correctly.\n\n\nFor instance, perhaps you are writing a tokenizer for simple algebraic\nexpressions, in which a sequence of letters will be seen as the\nmultiplication of variables (i.e., \"abc\" means \"a times b times c\") except\nfor a few special sequences of letters such as \"sin\" and \"cos\" and \"pi\" and\nperhaps a few others.\n\n\nIt is important to add the tokens for \"sin\" and \"cos\" and so on first, and\nthen the generic single-letter token thereafter, so that the special cases\nhave a chance to be applied before the general case.  Otherwise the general\ncase would catch all sequences of letters, and the special cases would\nnever have a chance to be applied.\n\n\nThe function signature looks like so:\n\n\nT.addType( regexp, formatter );\n\n\n\n\nThe two parameters are documented thoroughly in\n\nthe source code documentation\n, so I do not repeat the information here.\n\n\nTokenizing\n\n\nTo tokenize text, simply call \nT.tokenize( input )\n on the text.\n\n\nExample:\n\n\n\nT = new Tokenizer();\nT.addType( /sin/ );\nT.addType( /cos/ );\nT.addType( /pi/ );\nT.addType( /[a-z]/, function ( name ) { return \"Variable:\" + name; } );\nT.addType( /\\s/, function () { return null; } );\nconsole.log( T.tokenize( 'sin x' ) );\nconsole.log( T.tokenize( 'cospiy' ) );\n\n\n\n\nParsing\n\n\nA grammar is a set of rules defining the language to parse.  For more\ninformation on context-free grammars, see \nthe Wikipedia article on the\nEarley parser\n.\n\n\nFor the sake of having a concrete running example in this section, let's\nassume we want to want to create the extremely simple grammar used as \nan\nexample in that same\narticle\n.  We will make\none modification: we will accept any integer, rather than just the four\ndigits in that example.  Our grammar can thus be summarized as the following\nrules.\n\n\n\n\nP ::= S\n\n\nS ::= S+M | M\n\n\nM ::= M*T | T\n\n\nT ::= any integer\n\n\n\n\nWe can represent the same grammar without the | symbol by separating single\nlines into two separate lines.\n\n\n\n\nP ::= S\n\n\nS ::= S+M\n\n\nS ::= M\n\n\nM ::= M*T\n\n\nM ::= T\n\n\nT ::= any integer\n\n\n\n\nEither way of representing a grammar is acceptable, and supported by this\nmodule.  See \nthe section on specifying grammar\nrules\n, below.\n\n\nConstructor\n\n\nThere is just one constructor for grammars, and it takes as its sole\nargument the name of the start nonterminal.  This need not be a single\ncapital letter, as it is in the example above; nonterminals can have any\nword as their name.\n\n\nG = new Grammar( 'P' );\n\n\n\n\nSetting default options\n\n\nAfter constructing a new grammar, you can choose to set some default options\nthat will govern its behavior.  Any of these can be overridden in any call\nto the parse function, later, but you can set defaults here if you plan to\nneed them often.  You call \nG.setOption( name, value )\n to set the default\nvalue for any option.\n\n\nThe options are documented thoroughly in \nthe source code\ndocumentation\n,\nso I do not repeat that information here.  Examples of the output produced\nby the various options appears in \nthe parsing\nsection\n, below.\n\n\nAdding grammar rules\n\n\nA grammar rule requires a left-hand side, which must be a single\nnonterminal, represented by a string.  Its right-hand side is typically an\narray.  For instance, the grammar rule M ::= M\nT has M as its left-hand side\nand the three-element array M, \n, and T as its right hand side.\n\n\nThe elements on the right hand side come in two types.  There are other\nnonterminals (such as M and T) and there are terminals (the symbol \n, in\nthis example).  Nonterminals are represented as strings, and terminals as\nregular expressions.  Thus to create the grammar rule M ::= M\nT, we would\nuse \n'M'\n as the left-hand side and \n[ 'M', /\\*/, 'T' ]\n as the right-hand\nside.\n\n\nOur complete example grammar can then be created as follows.\n\n\nG = new Grammar( 'P' );\nG.addRule( 'P', [ 'S' ] );\nG.addRule( 'S', [ 'S', /\\+/, 'M' ] );\nG.addRule( 'S', [ 'M' ] );\nG.addRule( 'M', [ 'M', /\\*/, 'T' ] );\nG.addRule( 'M', [ 'T' ] );\nG.addRule( 'T', [ /-?[0-9]+/ ] );\n\n\n\n\nThere are a few things to improve upon here.\n\n\n\n\nWe may combine rules that have the same left-hand side, just be listing\n    the right-hand sides one after the other.\n\n\nWe may express a one-element array containing a terminal by the regular\n    expression alone, omitting the enclosing array.\n\n\nWe may express a right-hand side consisting exclusively of one or more\n    nonterminals by writing it as a single string, with the names of the\n    nonterminals separated by spaces.\n\n\n\n\nThis reduces our grammar somewhat.\n\n\nG = new Grammar( 'P' );\nG.addRule( 'P', 'S' );\nG.addRule( 'S', [ 'S', /\\+/, 'M' ], 'M' );\nG.addRule( 'M', [ 'M', /\\*/, 'T' ], 'T' );\nG.addRule( 'T', /-?[0-9]+/ );\n\n\n\n\nThe P in this grammar is redundant; we could have left it out and declared\nour grammar to start with S, but I leave it in to be consistent with the\nWikipedia article from which it was taken.\n\n\nRunning the parser\n\n\nWe can then parse text by passing it to the grammar's \nparse\n member\nfunction.\n\n\n\nT = new Tokenizer();\nT.addType( /-?[0-9]+/ );\nT.addType( /[+*]/ );\nT.addType( /\\s/, function () { return null; } );\nG = new Grammar( 'P' );\nG.setOption( 'tokenizer', T );\n// Comment out either of these lines and re-run to see their effects:\nG.setOption( 'addCategories', false );\nG.setOption( 'collapseBranches', true );\nG.addRule( 'P', 'S' );\nG.addRule( 'S', [ 'S', /\\+/, 'M' ], 'M' );\nG.addRule( 'M', [ 'M', /\\*/, 'T' ], 'T' );\nG.addRule( 'T', /-?[0-9]+/ );\nJSON.stringify( G.parse( '15 + -2 * 9' ), null, 2 );\n\n\n\n\nThe result of the \nparse\n function is an array of valid parsings.  In this\ncase, it has only one element, because there is only one way the expression\ncould be parsed.  For more complex grammars with ambiguities, more than one\nresult may be returned.\n\n\nTo override options chosen with \nsetOption()\n, call the parse function with\nan optional second argument, an object whose keys and values override the\nexisting options.  Example:\n\n\nG.parse( 'text', { collapseBranches : true } );\n\n\n\n\nMiscellany\n\n\nThis module extends the global \nJSON\n object with a routine that can compare\ntwo JSON structures for structural equality.  Two atomic values are equal if\nthey are actually equal, two arrays are equal if they have the same lengths\nand their corresponding entries are equal, and two objects are equal if they\nhave the same keys and the corresponding values in each are equal.\n\n\n\nJSON.equals( [ 1, 2, { 3 : 4 } ], [ 1, 2, { 3 : 4 } ] );\n\n\n\n\n\nJSON.equals( { a : 5 }, { b : 5 } );\n\n\n\n\nMore Examples\n\n\nIn addition to the brief examples shown in this file, \nthe test suite in the\nsource code\nrepository\n\nis (naturally) a large set of examples of how the module works.\n\n\n\n\n\n\nvar elements = document.getElementsByClassName( 'runnable-example' );\nfor ( var i = 0 ; i < elements.length ; i++ ) {\n    var source = elements[i].textContent;\n    elements[i].textContent = '';\n    var notebook = RunKit.createNotebook( {\n        element: elements[i],\n        source: source,\n        preamble: 'Tokenizer = require( \"earley-parser\" ).Tokenizer;\\nGrammar = require( \"earley-parser\" ).Grammar;'\n    } );\n}", 
            "title": "Reference"
        }, 
        {
            "location": "/api-reference/#api-reference", 
            "text": "", 
            "title": "API Reference"
        }, 
        {
            "location": "/api-reference/#getting-started", 
            "text": "", 
            "title": "Getting started"
        }, 
        {
            "location": "/api-reference/#in-the-browser", 
            "text": "Import the minified JavaScript, which you can  download from our repository\ndirectly \nor import from a CDN with the following one-liner.  script src='https://cdn.jsdelivr.net/npm/earley-parser@1/earley-parser.js' /script", 
            "title": "In the browser"
        }, 
        {
            "location": "/api-reference/#from-the-command-line", 
            "text": "Or install this package into your project the usual way:  npm install earley-parser  Then within any of your modules, import it as follows.  Tokenizer = require(  earley-parser  ).Tokenizer;\nGrammar = require(  earley-parser  ).Grammar;  After that, any of the example code snippets in this documentation should\nfunction as-is.", 
            "title": "From the command line"
        }, 
        {
            "location": "/api-reference/#tokenizing", 
            "text": "Traditionally, parsing text is split first into a \"tokenization\" phase, in\nwhich chunks of text are recognized as atomic units, and thus the string\nbecomes an array of its substrings, each of size 1 or greater, followed by\nthe parsing phase, which operates on that flat array, arranging it into a\ntree structure.  For example, in standard arithmetic, the text \"1+23*5\" might first be\ntokenized into the array  [ \"1\", \"+\", \"23\", \"*\", \"5\" ] , and then parsed\ninto the (prefix-notation) tree structure [ \"+\", \"1\", [ \"*\", \"23\", \"5\" ] ] .  A tokenizer is therefore an ordered list of regular expressions for\ndetecting tokens, popping them off of an input string, and then possibly\nmanipulating them before adding them to a growing array of tokens found in\nthe string.  This module provides a class for creating tokenizers.", 
            "title": "Tokenizing"
        }, 
        {
            "location": "/api-reference/#constructor", 
            "text": "The constructor takes no parameters, so it is very simple to use.  T = new Tokenizer();", 
            "title": "Constructor"
        }, 
        {
            "location": "/api-reference/#adding-types-to-a-tokenizer", 
            "text": "There is precisely one function for adding types to the tokenizer.  Note,\nhowever, that types are checked in the order in which they were added to the\ntokenizer, so when you call this function repeatedly to add various types of\ntokens, you should take care to order the calls correctly.  For instance, perhaps you are writing a tokenizer for simple algebraic\nexpressions, in which a sequence of letters will be seen as the\nmultiplication of variables (i.e., \"abc\" means \"a times b times c\") except\nfor a few special sequences of letters such as \"sin\" and \"cos\" and \"pi\" and\nperhaps a few others.  It is important to add the tokens for \"sin\" and \"cos\" and so on first, and\nthen the generic single-letter token thereafter, so that the special cases\nhave a chance to be applied before the general case.  Otherwise the general\ncase would catch all sequences of letters, and the special cases would\nnever have a chance to be applied.  The function signature looks like so:  T.addType( regexp, formatter );  The two parameters are documented thoroughly in the source code documentation , so I do not repeat the information here.", 
            "title": "Adding types to a tokenizer"
        }, 
        {
            "location": "/api-reference/#tokenizing_1", 
            "text": "To tokenize text, simply call  T.tokenize( input )  on the text.  Example:  \nT = new Tokenizer();\nT.addType( /sin/ );\nT.addType( /cos/ );\nT.addType( /pi/ );\nT.addType( /[a-z]/, function ( name ) { return \"Variable:\" + name; } );\nT.addType( /\\s/, function () { return null; } );\nconsole.log( T.tokenize( 'sin x' ) );\nconsole.log( T.tokenize( 'cospiy' ) );", 
            "title": "Tokenizing"
        }, 
        {
            "location": "/api-reference/#parsing", 
            "text": "A grammar is a set of rules defining the language to parse.  For more\ninformation on context-free grammars, see  the Wikipedia article on the\nEarley parser .  For the sake of having a concrete running example in this section, let's\nassume we want to want to create the extremely simple grammar used as  an\nexample in that same\narticle .  We will make\none modification: we will accept any integer, rather than just the four\ndigits in that example.  Our grammar can thus be summarized as the following\nrules.   P ::= S  S ::= S+M | M  M ::= M*T | T  T ::= any integer   We can represent the same grammar without the | symbol by separating single\nlines into two separate lines.   P ::= S  S ::= S+M  S ::= M  M ::= M*T  M ::= T  T ::= any integer   Either way of representing a grammar is acceptable, and supported by this\nmodule.  See  the section on specifying grammar\nrules , below.", 
            "title": "Parsing"
        }, 
        {
            "location": "/api-reference/#constructor_1", 
            "text": "There is just one constructor for grammars, and it takes as its sole\nargument the name of the start nonterminal.  This need not be a single\ncapital letter, as it is in the example above; nonterminals can have any\nword as their name.  G = new Grammar( 'P' );", 
            "title": "Constructor"
        }, 
        {
            "location": "/api-reference/#setting-default-options", 
            "text": "After constructing a new grammar, you can choose to set some default options\nthat will govern its behavior.  Any of these can be overridden in any call\nto the parse function, later, but you can set defaults here if you plan to\nneed them often.  You call  G.setOption( name, value )  to set the default\nvalue for any option.  The options are documented thoroughly in  the source code\ndocumentation ,\nso I do not repeat that information here.  Examples of the output produced\nby the various options appears in  the parsing\nsection , below.", 
            "title": "Setting default options"
        }, 
        {
            "location": "/api-reference/#adding-grammar-rules", 
            "text": "A grammar rule requires a left-hand side, which must be a single\nnonterminal, represented by a string.  Its right-hand side is typically an\narray.  For instance, the grammar rule M ::= M T has M as its left-hand side\nand the three-element array M,  , and T as its right hand side.  The elements on the right hand side come in two types.  There are other\nnonterminals (such as M and T) and there are terminals (the symbol  , in\nthis example).  Nonterminals are represented as strings, and terminals as\nregular expressions.  Thus to create the grammar rule M ::= M T, we would\nuse  'M'  as the left-hand side and  [ 'M', /\\*/, 'T' ]  as the right-hand\nside.  Our complete example grammar can then be created as follows.  G = new Grammar( 'P' );\nG.addRule( 'P', [ 'S' ] );\nG.addRule( 'S', [ 'S', /\\+/, 'M' ] );\nG.addRule( 'S', [ 'M' ] );\nG.addRule( 'M', [ 'M', /\\*/, 'T' ] );\nG.addRule( 'M', [ 'T' ] );\nG.addRule( 'T', [ /-?[0-9]+/ ] );  There are a few things to improve upon here.   We may combine rules that have the same left-hand side, just be listing\n    the right-hand sides one after the other.  We may express a one-element array containing a terminal by the regular\n    expression alone, omitting the enclosing array.  We may express a right-hand side consisting exclusively of one or more\n    nonterminals by writing it as a single string, with the names of the\n    nonterminals separated by spaces.   This reduces our grammar somewhat.  G = new Grammar( 'P' );\nG.addRule( 'P', 'S' );\nG.addRule( 'S', [ 'S', /\\+/, 'M' ], 'M' );\nG.addRule( 'M', [ 'M', /\\*/, 'T' ], 'T' );\nG.addRule( 'T', /-?[0-9]+/ );  The P in this grammar is redundant; we could have left it out and declared\nour grammar to start with S, but I leave it in to be consistent with the\nWikipedia article from which it was taken.", 
            "title": "Adding grammar rules"
        }, 
        {
            "location": "/api-reference/#running-the-parser", 
            "text": "We can then parse text by passing it to the grammar's  parse  member\nfunction.  \nT = new Tokenizer();\nT.addType( /-?[0-9]+/ );\nT.addType( /[+*]/ );\nT.addType( /\\s/, function () { return null; } );\nG = new Grammar( 'P' );\nG.setOption( 'tokenizer', T );\n// Comment out either of these lines and re-run to see their effects:\nG.setOption( 'addCategories', false );\nG.setOption( 'collapseBranches', true );\nG.addRule( 'P', 'S' );\nG.addRule( 'S', [ 'S', /\\+/, 'M' ], 'M' );\nG.addRule( 'M', [ 'M', /\\*/, 'T' ], 'T' );\nG.addRule( 'T', /-?[0-9]+/ );\nJSON.stringify( G.parse( '15 + -2 * 9' ), null, 2 );  The result of the  parse  function is an array of valid parsings.  In this\ncase, it has only one element, because there is only one way the expression\ncould be parsed.  For more complex grammars with ambiguities, more than one\nresult may be returned.  To override options chosen with  setOption() , call the parse function with\nan optional second argument, an object whose keys and values override the\nexisting options.  Example:  G.parse( 'text', { collapseBranches : true } );", 
            "title": "Running the parser"
        }, 
        {
            "location": "/api-reference/#miscellany", 
            "text": "This module extends the global  JSON  object with a routine that can compare\ntwo JSON structures for structural equality.  Two atomic values are equal if\nthey are actually equal, two arrays are equal if they have the same lengths\nand their corresponding entries are equal, and two objects are equal if they\nhave the same keys and the corresponding values in each are equal.  \nJSON.equals( [ 1, 2, { 3 : 4 } ], [ 1, 2, { 3 : 4 } ] );  \nJSON.equals( { a : 5 }, { b : 5 } );", 
            "title": "Miscellany"
        }, 
        {
            "location": "/api-reference/#more-examples", 
            "text": "In addition to the brief examples shown in this file,  the test suite in the\nsource code\nrepository \nis (naturally) a large set of examples of how the module works.   \nvar elements = document.getElementsByClassName( 'runnable-example' );\nfor ( var i = 0 ; i < elements.length ; i++ ) {\n    var source = elements[i].textContent;\n    elements[i].textContent = '';\n    var notebook = RunKit.createNotebook( {\n        element: elements[i],\n        source: source,\n        preamble: 'Tokenizer = require( \"earley-parser\" ).Tokenizer;\\nGrammar = require( \"earley-parser\" ).Grammar;'\n    } );\n}", 
            "title": "More Examples"
        }
    ]
}